import requests
import time
from datetime import datetime
from src.models.base import Session, Trend, TrendMetric, init_db

# Configuration des Sources (Subreddits cl√©s pour tes niches)
SOURCES = {
    'Cinema': ['movies', 'boxoffice', 'netflix', 'cine'],
    'Sport': ['soccer', 'nba', 'formula1', 'ligue1'],
    'Music': ['popheads', 'hiphopheads', 'music', 'kpop']
}

# User-Agent personnalis√© OBLIGATOIRE pour ne pas √™tre bloqu√© par Reddit
HEADERS = {
    "User-Agent": "ViralWatchBot/1.0 (by /u/TesBesoinsDeData)"
}

def fetch_subreddit_hot(subreddit):
    """R√©cup√®re les posts 'Hot' d'un subreddit via l'API JSON publique"""
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=20"
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        
        if response.status_code == 429:
            print(f"‚ö†Ô∏è Trop de requ√™tes pour r/{subreddit}. Pause...")
            time.sleep(2)
            return []
            
        if response.status_code != 200:
            print(f"‚ùå Erreur r/{subreddit}: {response.status_code}")
            return []

        data = response.json()
        posts = []
        
        # Navigation dans le JSON Reddit: data -> children -> data
        for item in data.get('data', {}).get('children', []):
            post = item['data']
            
            # On ignore les posts √©pingl√©s (souvent des r√®gles, pas des trends)
            if post.get('stickied'):
                continue
                
            posts.append({
                'title': post['title'],
                'score': post['score'],       # Net Upvotes
                'comments': post['num_comments'], # Volume de discussion
                'url': post['url'],
                'created_utc': post['created_utc']
            })
            
        return posts

    except Exception as e:
        print(f"‚ùå Erreur Exception r/{subreddit}: {e}")
        return []

def process_reddit_trends():
    session = Session()
    print("üöÄ D√©marrage du scan Reddit...")

    total_new = 0
    
    for niche, subreddits in SOURCES.items():
        print(f"\n--- Analyse Niche: {niche} ---")
        
        for sub in subreddits:
            posts = fetch_subreddit_hot(sub)
            print(f"  r/{sub}: {len(posts)} posts r√©cup√©r√©s")
            
            for post in posts:
                topic = post['title'][:250] # On tronque pour la DB
                
                # Le score de v√©locit√© brut ici est simple : Score + Commentaires
                # (Dans la V2 on divisera par le temps √©coul√© depuis le post)
                virality_score = post['score'] + post['comments']
                
                # Filtre : On ne garde que ce qui a un minimum d'impact
                if virality_score < 100:
                    continue

                # 1. Upsert Trend
                trend_obj = session.query(Trend).filter_by(topic=topic).first()
                
                if not trend_obj:
                    trend_obj = Trend(topic=topic, niche=niche)
                    session.add(trend_obj)
                    total_new += 1
                    session.commit() # Commit pour avoir l'ID
                
                # 2. Ajout M√©trique
                metric = TrendMetric(
                    trend_id=trend_obj.id,
                    platform='Reddit',
                    volume=post['score'], # On utilise le score comme proxy du volume
                    velocity_score=float(virality_score)
                )
                session.add(metric)
            
            # Pause √©thique pour respecter l'API
            time.sleep(1)
    
    session.commit()
    session.close()
    print(f"\n‚úÖ Ingestion termin√©e. {total_new} nouveaux sujets d√©tect√©s.")

if __name__ == "__main__":
    init_db()
    process_reddit_trends()